name: Scan Website for Ads

on:
  workflow_dispatch:
    inputs:
      website_url:
        description: 'URL của website cần quét'
        required: true
        type: string
      scan_depth:
        description: 'Độ sâu quét (1-5)'
        required: false
        default: '1'
        type: string  # Changed to string


jobs:
  scan-ads:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 selenium webdriver-manager tqdm

      - name: Scan website for ads
        id: scan
        run: |
          python - <<EOF
          import re
          import time
          import logging
          from bs4 import BeautifulSoup
          import requests
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from selenium.webdriver.common.by import By
          from webdriver_manager.chrome import ChromeDriverManager
          from urllib.parse import urljoin, urlparse
          from tqdm import tqdm
          import concurrent.futures
          import hashlib
          import json
          import ast

          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          def get_domain(url):
              return urlparse(url).netloc

          def extract_ad_elements(soup, current_url, ad_elements):
            ad_patterns = [
                "(?:ads?|advert(?:isement)?|quangcao)[-_]?(?:preload|box|slot|unit|banner|container|wrapper|frame|block|space|zone|spot|content|display|holder)",
                "banner[-_]?(?:preload|top|bottom|left|right|side|float|sticky|fixed|mobile|main|center|ads?)",
                "(?:catfish|float|sticky|overlay|modal|popup|lightbox)[-_]?(?:ads?|banner|box|container|bottom|top|left|right|mobile)",
                "(?:google|gg|fb|facebook|twitter|tiktok)[-_]?(?:ads?|adsense|pixel|tracking)",
                "(?:tracking|analytics|pixel|remarketing|affiliate|conversion)[-_]?(?:box|container|unit|pixel|tag)",
                "(?:promo|marketing|campaign|special|sponsor|partner)[-_]?(?:box|banner|container|content|section)",
                "(?:notification|alert|message|cookie|gdpr|privacy)[-_]?(?:banner|popup|modal|box|consent)",
                "(?:video|player|media|jw)[-_]?(?:ads?|overlay|banner|container|cue)",
                "(?:social|share|follow|community)[-_]?(?:buttons?|container|widget|plugin|feed)",
                "(?:newsletter|subscribe|chat|messenger|support)[-_]?(?:popup|modal|form|box|widget)",
                "(?:mobile|responsive)[-_]?(?:menu|nav|header|ads?|banner)",
                "(?:sidebar|widget|toolbar)[-_]?(?:container|wrapper|fixed|ads?)",
                "(?:aswift|adsbygoogle|adskeeper|adnow|admicro|adtima)[-_]?\\d*"
              ]
            script_patterns = [
                "(?:ads?|adv|analytics|track)(?:ertis(?:ing|ements?)|manager|loader|config|pixel|tag|event)?",
                "(?:google|facebook|fb|tiktok)[-_]?(?:analytics|tag|ads?|adsense|pixel|sdk)",
                "(?:hotjar|optimizely|segment|mixpanel|chartbeat|crazyegg|comscore)",
                "(?:retargeting|remarketing|affiliate)[-_]?(?:code|pixel|tracking)",
                "(?:social|share|chat|push|notification)[-_]?(?:buttons?|widget|sdk)"
              ]
            media_patterns = [
                "(?:video|player)[-_]?(?:embed|frame|container)",
                "(?:map|location|payment|survey)[-_]?(?:embed|frame|container)",
                "banner[-_]?(?:image|img|ad)",
                 "(?:background|bg|header|footer)[-_]?(?:ad|banner)"
              ]
            style_patterns = [
                "position:\\s*(?:fixed|sticky)",
                "z-index:\\s*[0-9]{4,}",
                "(?:width|height):\\s*100(?:vh|vw|%)",
                "display:\\s*(?:flex|grid)",
                "opacity:\\s*[01](?:\\.[0-9]+)?"
              ]
            
            def process_element(el, selector_type=''):
                  if not el.name:
                      return None
                      
                  selector_parts = [current_url, "##", el.name]
                  
                  if 'id' in el.attrs:
                      selector_parts.append(f"#{el['id']}")
                  elif 'class' in el.attrs:
                      classes = el['class'] if isinstance(el['class'], list) else [el['class']]
                      filtered_classes = [c for c in classes if len(c) > 2]
                      if filtered_classes:
                          selector_parts.append(f".{'.'.join(filtered_classes)}")
                          
                  special_attrs = ['data-id', 'data-name', 'role', 'data-ad', 'data-type']
                  for attr in special_attrs:
                      if attr in el.attrs:
                          selector_parts.append(f"[{attr}={el[attr]}]")
                          
                  selector = ''.join(selector_parts)
                  if selector_type:
                      selector += f"${selector_type}"
                  return selector
                  
            for pattern in ad_patterns:
                elements = soup.find_all(lambda tag: any(
                    re.search(pattern, str(attr_value), re.I)
                    for attr_name, attr_value in tag.attrs.items()
                    if isinstance(attr_value, (str, list))
                ))
                
                for el in elements:
                    selector = process_element(el)
                    if selector:
                        ad_elements.add(selector)

            for script in soup.find_all('script', src=True):
                src = script['src'].lower()
                if any(re.search(p, src, re.I) for p in script_patterns):
                    domain = urlparse(src).netloc
                    if domain:
                        ad_elements.add(f"||{domain}^$script,third-party")

            for el in soup.find_all(['iframe', 'embed', 'img', 'picture', 'source']):
                src = el.get('src', '').lower()
                if src and any(re.search(p, src, re.I) for p in media_patterns):
                    domain = urlparse(src).netloc
                    if domain:
                        el_type = 'subdocument' if el.name in ['iframe', 'embed'] else 'image'
                        ad_elements.add(f"||{domain}^${el_type},third-party")
            
            for div in soup.find_all(['div', 'section', 'aside'], style=True):
                  style = div['style'].lower()
                  if any(re.search(p, style, re.I) for p in style_patterns):
                      selector = process_element(div)
                      if selector:
                          ad_elements.add(selector)


          def scan_website(url, max_depth):
              chrome_options = Options()
              chrome_options.add_argument('--headless=new')
              chrome_options.add_argument('--no-sandbox')
              chrome_options.add_argument('--disable-dev-shm-usage')
              chrome_options.add_argument('--disable-gpu')
              chrome_options.add_argument('--window-size=1920,1080')
              chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
              
              service = Service(ChromeDriverManager().install())
              driver = webdriver.Chrome(service=service, options=chrome_options)
              wait = WebDriverWait(driver, 10)
              
              ad_elements = set()
              scanned_urls = set()
              base_domain = get_domain(url)
              urls_to_scan = [(url, 0)]
              
              try:
                  with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                      future_to_url = {}
                      
                      for current_url, depth in tqdm(urls_to_scan):
                          if depth >= max_depth or current_url in scanned_urls:
                              continue
                              
                          scanned_urls.add(current_url)
                          logging.info(f'Đang quét: {current_url}')
                          
                          try:
                              driver.get(current_url)
                              time.sleep(2)
                              
                              scroll_count = 0
                              max_scrolls = 5
                              while scroll_count < max_scrolls:
                                  driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                                  time.sleep(1)
                                  
                                  new_height = driver.execute_script("return document.body.scrollHeight")
                                  if scroll_count > 0 and new_height == last_height:
                                      break
                                  last_height = new_height
                                  scroll_count += 1
                              
                              load_more_patterns = [
                                  "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'load more')]",
                                  "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'xem thêm')]",
                                  "//a[contains(@class, 'load-more')]",
                                  "//div[contains(@class, 'load-more')]"
                              ]
                              
                              for pattern in load_more_patterns:
                                  try:
                                      buttons = driver.find_elements(By.XPATH, pattern)
                                      for button in buttons[:3]:
                                          if button.is_displayed():
                                              button.click()
                                              time.sleep(1)
                                  except:
                                      continue
                              
                              page_source = driver.page_source
                              
                              if depth < max_depth - 1:
                                  soup = BeautifulSoup(page_source, 'html.parser')
                                  links = soup.find_all('a', href=True)
                                  for link in links:
                                      next_url = urljoin(current_url, link['href'])
                                      if get_domain(next_url) == base_domain and next_url not in scanned_urls:
                                          urls_to_scan.append((next_url, depth + 1))
                              
                              future = executor.submit(extract_ad_elements, 
                                                          BeautifulSoup(page_source, 'html.parser'),
                                                          current_url, 
                                                          ad_elements)
                              future_to_url[future] = current_url
                                      
                          except Exception as e:
                              logging.error(f'Lỗi khi quét {current_url}: {str(e)}')
                              continue
                          
                      for future in concurrent.futures.as_completed(future_to_url):
                          url = future_to_url[future]
                          try:
                              future.result()
                          except Exception as e:
                              logging.error(f'Lỗi khi xử lý {url}: {str(e)}')
                          
              finally:
                  driver.quit()
                  
              filtered_rules = set()
              seen_hashes = set()
              
              for rule in ad_elements:
                  rule = rule.strip()
                  if rule and len(rule) > 1:
                      rule = re.sub(r'\s+', ' ', rule)
                      rule = re.sub(r'[\u200b-\u200f\u2028-\u202f\u205f-\u206f]', '', rule)
                      rule_hash = hashlib.md5(rule.encode()).hexdigest()
                      
                      if rule_hash not in seen_hashes:
                          seen_hashes.add(rule_hash)
                          filtered_rules.add(rule)
                      
              return sorted(list(filtered_rules))

          url = "${{ github.event.inputs.website_url }}"
          max_depth = int("${{ github.event.inputs.scan_depth }}")
          logging.info(f'Bắt đầu quét {url} với độ sâu {max_depth}')
          ad_rules = scan_website(url, max_depth)
          logging.info(f'Đã tìm thấy {len(ad_rules)} rules')

          with open('new_rules.txt', 'w', encoding='utf-8') as f:
              f.write('\n'.join(ad_rules))
          EOF

      - name: Create Pull Request
        run: |
          if [ -s new_rules.txt ]; then
              git pull origin main
              
              # Tạo temporary file để lưu rules mới
              touch temp_rules.txt
              
              # Đọc rules hiện tại và rules mới, loại bỏ trùng lặp
              if [ -f Yuusei.txt ]; then
                grep -v '^#' Yuusei.txt | grep -v '^$' > temp_rules.txt
              fi
              cat new_rules.txt >> temp_rules.txt
              
              # Sắp xếp và loại bỏ trùng lặp
              sort -u temp_rules.txt > unique_rules.txt
              
              # Tạo file mới với header
              {
                echo "! Title: Yuusei Filter List"
                echo "! Description: Bộ lọc quảng cáo cho người Việt"
                echo "! Version: $(date +'%d-%m-%Y')"
                echo "! Last modified: $(date +'%d-%m-%Y %H:%M:%S')"
                echo "! Expires: 1 days"
                echo "! Homepage: https://github.com/JustaTama/Yuusei-Extensions"
                echo "! License: https://github.com/JustaTama/Yuusei-Extensions/blob/main/LICENSE"
                echo ""
                echo "! Rules từ ${{ github.event.inputs.website_url }} - $(date +'%Y-%m-%d')"
                cat unique_rules.txt
                echo ""
              } > Yuusei.txt
              
              # Cleanup
              rm temp_rules.txt unique_rules.txt
              
              new_rules=$(wc -l < new_rules.txt)
              
              git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
              git config --local user.name "github-actions[bot]"
              git add Yuusei.txt
              git commit -m "✨ Thêm $new_rules rules từ ${{ github.event.inputs.website_url }}
              
              - Website: ${{ github.event.inputs.website_url }}
              - Độ sâu quét: ${{ github.event.inputs.scan_depth }}
              - Số rules mới: $new_rules
              - Thời gian: $(date +'%Y-%m-%d %H:%M:%S')"
          
              # Sanitize the website_url to create a valid branch name
              SANITIZED_URL=$(echo "${{ github.event.inputs.website_url }}" | sed 's/[^a-zA-Z0-9_-]/_/g')
              BRANCH_NAME="scan-ads-${SANITIZED_URL}-${{ github.run_id }}"
              git checkout -b "$BRANCH_NAME"

              git push origin "$BRANCH_NAME"
              
              PR_URL=$(gh pr create --base main --title "✨ Thêm rules mới từ ${{ github.event.inputs.website_url }}" --body "
              - Website: ${{ github.event.inputs.website_url }}
              - Độ sâu quét: ${{ github.event.inputs.scan_depth }}
              - Số rules mới: $new_rules
              - Thời gian: $(date +'%Y-%m-%d %H:%M:%S')" | sed -n 's/.*\(https:\/\/github.com\/.*\/pull\/\)/\1/p')
              
              echo "Pull request URL: $PR_URL"
          else
              echo "No new rules found, skipping pull request creation."
          fi
        env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
